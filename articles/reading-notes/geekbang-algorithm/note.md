# 极客时间：数据结构与算法之美
## 01 Why
- 通关大厂面试
- 掌握数据结构和算法，不管对于阅读框架源码，还是理解其背后的设计思想都是非常有用的
- 有追求，不想被行业淘汰，就不能只会写凑合的代码，性能好坏是好代码其中一个要求
## 02 How
抓住重点
### 定义
- 广义
    数据结构就是指一组数据的储存结构，算法就是操作数据的一组方法
- 狭义
    某些著名的数据结构和算法，比如队列、堆、栈、二分查找等等。利用这些前人的智慧可以高效地帮助我们解决实际开发问题

数据结构是为算法服务的，算法要作用于特定的数据结构上。二者无法孤立。

### 学习重点

- 掌握复杂度分析
- 十个数据结构
    数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie 树
- 十个算法
    递归、排序、二分查找、搜索、哈希、贪心、分治、回溯、动态规划、字符串匹配

要学习算法和数据结构的来历、自身特点、适应解决的问题以及实际的应用场景

### 学习技巧
- 边学边练，适度刷题
- 多问，多思考，多互动
- 打怪升级学习法
- 知识需要沉淀，不要企图一下子掌握所有

## 03 复杂度分析（上）

复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半

### 事后统计法

跑一遍代码，通过统计、监控来得到算法执行的时间和占用的内存大小。

缺点
- 测试结果非常依赖测试环境
- 搜数据规模的影响很大

### 大 O 复杂度表示法
所有代码的执行时间 T(n) 与每行代码的执行次数 n 成正比

T(n) = O(f(n))

T(n) 表示代码执行时间
n 表示数据规模
f(n) 表示所有代码的执行次数总和
O 表示代码的执行时间 T(n) 与 f(n) 成正比

大 O 时间复杂度实际上并不表示代码的真正执行时间，而是表示**代码执行时间随数据规模增长的变化趋势**，也被称为**渐进时间复杂度**，简称**时间复杂度**

### 时间复杂度分析

#### 1. 只关注循环执行次数最多的代码

由于大 O 浮渣度分析只是表示一种变化趋势，会忽略掉公式里的常量、低阶、系数，只记录最大阶的量级。因此分析时间复杂度的时候，只关注循环执行次数最多的代码就可以了。

#### 2. 加法法则：总的复杂度等于量级最大的那段代码的复杂度
如果 T1(n)=O(f(n))，T2(n)=O(g(n))；那么 T(n)=T1(n)+T2(n)=max(O(f(n)), O(g(n))) =O(max(f(n), g(n))).
#### 3. 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积
 T1(n)=O(f(n))，T2(n)=O(g(n))；那么 T(n)=T1(n)*T2(n)=O(f(n))*O(g(n))=O(f(n)*g(n)).
### 常见时间复杂度分析实例

![](./images/3723793cc5c810e9d5b06bc95325bf0a.jpg)

分为两类，多项式量级和非多项式量级。其中，非多项式量级只有两个：O(2^n) 和 O(n!)。

当数据规模 n 越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。
#### 多项式量级
##### 1. O(1)
一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万的代码也是 O(1)
##### 2.  O(log n)、O(n log n)
```
 i=1;
 while (i <= n)  {
   i = i * 2;
 }
```
设 x 为执行次数， 则 2 的 x 次方等于 n, 因此 x = log2n。
采用大 O 变价复杂度的时候，对于对数阶的时间复杂度常忽略对数的底，统一表示为 O(logn)
##### 3. O(m+n)、O(m*n)
代码的复杂度由两个数据的规模来决定.

加法规则改为：T1(m) + T2(n) = O(f(m) + g(n))。但是乘法法则继续有效：T1(m)*T2(n) = O(f(m) * f(n))。

### 空间复杂度分析

空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系。

## 03 复杂度分析（下）

### 复杂度分析的 4 个概念
1. 最坏情况时间复杂度：代码在最理想情况下执行的时间复杂度。
2. 最好情况时间复杂度：代码在最坏情况下执行的时间复杂度。
3. 平均时间复杂度：用代码在所有情况下执行的次数的加权平均值表示。
4. 均摊时间复杂度：在代码执行的所有复杂度情况中绝大部分是低级别的复杂度，个别情况是高级别复杂度且发生具有时序关系时，可以将个别高级别复杂度均摊到低级别复杂度上。基本上均摊结果就等于低级别复杂度。

### 平均情况时间复杂度

查找变量 x 是否在 容量为 n 的数组中

我们知道，要查找的变量 x，要么在数组里，要么就不在数组里。假设在数组中与不在数组中的概率都为 1/2。另外，要查找的数据出现在 0～n-1 这 n 个位置的概率也是一样的，为 1/n。所以，根据概率乘法法则，要查找的数据出现在 0～n-1 中任意位置的概率就是 1/(2n)。

平均时间复杂度就这样计算

![](./images/36c0aabdac69032f8a43368f5e90c67f.jpg)

这就是概率论里的加权平均数，也称为期望值，所以平均时间复杂度全称是加权平均时间复杂度或者期望时间复杂度

### 均摊时间复杂度

对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上。

这种复杂度分析的方法称为**摊还分析**，通过摊还分析得到的时间复杂度我们起了一个名字，叫均摊时间复杂度。

在能够应用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度。

均摊时间复杂度就是一种特殊的平均时间复杂度

### 如何分析平均、均摊时间复杂度？
1. 平均时间复杂度
代码在不同情况下复杂度出现量级差别，则用代码所有可能情况下执行次数的加权平均值表示。
2. 均摊时间复杂度
两个条件满足时使用：1）代码在绝大多数情况下是低级别复杂度，只有极少数情况是高级别复杂度；2）低级别和高级别复杂度出现具有时序规律。均摊结果一般都等于低级别复杂度。

## 04 数据结构与算法学习书单

入门：大话数据结构、算法图解

面试：剑指offer

经典：算法

## 05 数组

### 定义

**数组是一种_线性表_数据结构。它用一组_连续的内存空间_来存储一组具有_相同类型_的数据。**

### 随机访问

数组支持随机访问，根据下标随机访问的时间复杂度是 O(1)。

#### 如何实现根据下标随机访问数组元素？

通过寻址公式，计算出该元素存储的内存地址：
```
a[i]_address = base_address + i * data_type_size
```

### 低效的“插入”和“删除”

#### 插入：
若有一元素想往 int[n] 的第 k 个位置插入数据，需要在 k-n 的位置往后移。

最好情况时间复杂度 O(1)
最坏情况复杂度为 O(n)
平均负责度为 O(n)

如果数组中的数据不是有序的，也就是无规律的情况下，可以直接把第 k 个位置上的数据移到最后，然后将插入的数据直接放在第 k 个位置上。这样时间复杂度就将为 O（1）了。

#### 删除：
与插入类似，为了保持内存的连续性。

最好情况时间复杂度 O(1)
最坏情况复杂度为 O(n)
平均负责度为 O(n)

提高效率：将多次删除操作中集中在一起执行，可以先记录已经删除的数据，但是不进行数据迁移，而仅仅是记录，当发现没有更多空间存储时，再执行真正的删除操作。这也是 JVM 标记清除垃圾回收算法的核心思想。

### 警惕数组的访问越界问题

数组越界在 C 语言中是一种未决行为，并没有规定数组访问越界时编译器应该如何处理。 不同的语言对数组访问越界的处理方式不同，即便是同一种语言，不同的编译器处理的方式也不同。

### 容器与数组的选择

1. 存储基本数据类型，或者更关注性能可以使用数组
2. 数据大小已知，并且对数据的操作简单，可以使用数组
3. 多维数组的时候，数组更加直观
4. 业务开发，使用容器足够，开发框架，追求性能，首先数组

### 为什么大多数数组从 0 开始编号

1. 数组的根据下标访问元素是根据计算公式来计算内存偏移，从 0 开始更合理
2. 习惯问题

### JVM 标记清除算法
在标记阶段会标记所有的可访问的对象，在清除阶段会遍历堆，回收那些没有被标记的对象。现在想想，和「如果数组中的数据不是有序的，也就是无规律的情况下，可以直接把第k个位置上的数据移到最后，然后将插入的数据直接放在第k个位置上。」思想类似。

### 二维数组内存寻址

对于 `m * n` 的数组，`a [ i ][ j ] (i < m,j < n)`的地址为：
```
address = base_address + ( i * n + j) * type_size
```

## 06 链表（上）

### 定义

通过指针将一组零散的内存块串联起来就构成了链表。

其中这些内存块称为**节点**，节点中记录链表下个节点地址的指针称为**后继指针**。

两个特殊节点：第一个节点称为**头结点**，最后一个节点称为**尾节点**，尾节点的后继节点是一个**空地址 NULL**

### 分类
1. 单链表 单链表的节点只包含一个存储数据的内存空间和后继指针 next
2. 双向链表 双向链表的节点包含一个存储数据的内存空间以及前驱指针 prev 和后继指针 next
3. 循环链表 尾节点的后继指针指向头节点的链表就是循环链表

### 双向链表的优点
链表遍历的时间复杂度是 O(n),在实际使用中双向链表可以减少链表的遍历操作。

### 链表与数组性能对比
#### 数组
- 随机访问 O(1) 插入删除 O(n)
- 实际使用中由于数组使用的是连续内存空间，可以借助 CPU 的缓存机制预读数组中的数据，因此访问效率更高。
- 数组扩容非常耗时
#### 链表
- 随机访问 O(n) 插入删除 O(1)
- 由于不是连续内存，对 CPU 缓存不太友好。
- 更容易造成内存碎片
- 对于 Java 来说还会导致频繁的垃圾回收。

### 基于链表实现 LRU 缓存淘汰算法

1. 当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。
2. 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。
3. 如果此数据没有在缓存链表中，又可以分为两种情况：
    * 如果此时缓存未满，则将此结点直接插入到链表的头部；
    * 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。


## 07 链表（下）

### 写链表代码的技巧

1. 理解指针或引用的含义

    将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。
2. 警惕指针丢失和内存泄漏
    - 插入结点时，一定要注意操作的顺序
    - 删除链表结点时，也一定要记得手动释放内存空间（自动管理内存的语言除外）
3. 利用哨兵简化实现难度

    链表中的“哨兵”节点是解决边界问题的，不参与业务逻辑。如果我们引入“哨兵”节点，则不管链表是否为空，head指针都会指向这个“哨兵”节点。
    
    我们把这种有“哨兵”节点的链表称为带头链表，相反，没有“哨兵”节点的链表就称为不带头链表。
4. 重点留意边界条件处理
    * 如果链表为空时，代码是否能正常工作？
    * 如果链表只包含一个结点时，代码是否能正常工作？
    * 如果链表只包含两个结点时，代码是否能正常工作？
    * 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？
5. 举例画图，辅助思考

6. 多写多练，没有捷径

### 常见链表题

* 单链表反转
* 链表中环的检测
* 两个有序的链表合并
* 删除链表倒数第 n 个结点
* 求链表的中间结点

### tips
- 利用 LeetCode 刷题

    LeetCode上的测试用例往往很全，能测试出特殊情况下的 bug
- 不用太纠结

    在一定时间内实在想不出来就去搜一下答案，没必要一直纠结。有些巧妙的解题思路确实不是很好想出来的。

## 08 栈

### 定义
#### 什么是栈
- 后进者先出，先进者后出，这就是典型的“栈”结构。
- 从栈的操作特性来看，是一种“操作受限”的线性表，只允许在端插入（push）和删除数据（pop）。
#### 为什么需要栈？
- 栈是一种操作受限的数据结构，其操作特性用数组和链表均可实现。
- 但，任何数据结构都是对特定应用场景的抽象，数组和链表虽然使用起来更加灵活，但却暴露了几乎所有的操作，难免会引发错误操作的风险。
- 所以，当某个数据集合只涉及在某端插入和删除数据，且满足后进者先出，先进者后出的操作特性时，我们应该首选栈这种数据结构。
### 实现
- 用数组实现的栈，我们叫作顺序栈
- 用链表实现的栈，我们叫作链式栈

#### 复杂度

不管是顺序栈还是链式栈，入栈、出栈只涉及栈顶个别数据的操作，所以时间复杂度都是 O(1)

在入栈和出栈过程中，只需要一两个临时变量存储空间，所以复杂度是 O(1)

**空间复杂度的时候，是指除了原本的数据存储空间外，算法运行还需要额外的存储空间。**

#### 支持动态扩容的顺序栈

根据摊还分析法，每次需要扩容的 push 的复杂度可以均摊到普通的不需要扩容时 push，因此其均摊时间复杂度是 O(1)

### 应用

#### 1.函数调用
操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构，用来存储函数调用时的临时变量。

每进入一个函数，就会将其中的临时变量作为栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。
#### 2.表达式
利用两个栈，其中一个用来保存操作数，另一个用来保存运算符。

我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较，若比运算符栈顶元素优先级高，就将当前运算符压入栈，若比运算符栈顶元素的优先级低或者相同，从运算符栈中取出栈顶运算符，从操作数栈顶取出2个操作数，然后进行计算，把计算完的结果压入操作数栈，继续比较。
#### 3.括号匹配
用栈保存为匹配的左括号，从左到右一次扫描字符串，当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号，如果能匹配上，则继续扫描剩下的字符串。

如果扫描过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。

当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明未匹配的左括号为非法格式。
#### 4.浏览器的前进后退
使用两个栈 A 和 B，浏览过程中把浏览的界面按顺序压入栈 A 中。

点击返回的按钮时，让栈 A 栈顶出栈，出栈的数据压入栈 B 中。

点击前进按钮时， 让栈 B 栈顶出栈，出栈的数据压入栈 A。

栈 A 为空说明无法后退， 栈 B 为空说明无法前进。

### QA
#### 1.为什么函数调用要用“栈”来保存临时变量呢？
函数调用的局部状态之所以用栈来记录是因为这些数据的存活时间满足“后入先出”（LIFO）顺序，而栈的基本操作正好就是支持这种顺序的访问。

函数的调用有完美的嵌套关系——调用者的生命期总是长于被调用者的生命期，并且后者在前者的之内。这样，被调用者的局部信息所占空间的分配总是后于调用者的（后入），而其释放则总是先于调用者的（先出），所以正好可以满足栈的LIFO顺序，选用栈这种数据结构来实现调用栈是一种很自然的选择。

[知乎 RednaxelaFX 的 回答](https://www.zhihu.com/question/34499262/answer/59415153)

#### 2.内存中的堆栈和数据结构堆栈是不是一个概念？
内存中的堆栈和数据结构堆栈不是一个概念，可以说内存中的堆栈是真实存在的物理区，数据结构中的堆栈是抽象的数据存储结构
### LeetCode 
20,155,232,844,224,682,496gi

## 09 队列

### 定义
- 先进先出的就是队列
- 与栈一样也是一个操作受限的线性表数据额结构：队列头部取数据（enqueue），队列尾部插入数据（dequeue）

### 实现
- 用数组实现的队列叫作顺序队列
- 用链表实现的队列叫作链式队列
- 为了避免顺序队列的数据搬移，有了循环队列的解决思路

### 循环队列
当顺序队列的数组空间用完之后（tail 等于数组的大小），新插入的数据插入到数组头部。这样就避免了数据迁移。

循环队列的实现关键是 **确定队空和队满的判断**

- 队空：head 等于 tail
- 队满：（tail + 1）%  n == head

表达式是怎么来的：在一般情况下，我们可以看出来，当队列满时，tail+1=head。但是，有个特殊情况，就是tail=n-1，而head=0时，这时候，tail+1=n，而head=0，所以用(tail+1)%n == n%n == 0。而且，tail+1最大的情况就是 n ，不会大于 n，这样，tail+1 除了最大情况，不然怎么余 n 都是 tail+1 本身，也就是 head。这样，表达式就出现了。

### 阻塞队列和并发队列

- 阻塞队列：在队列基础上增加了阻塞操作。队列为空的时候从堆头取数据被阻塞，队列满了之后插入数据被阻塞
- 并发队列：能保证多线程并发操作时线程安全的队列称为并发队列

### 应用
对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。

### 无锁并发队列
考虑使用CAS实现无锁队列。在入队前，获取tail位置，入队时比较tail是否发生变化，如果否，则允许入队，反之，本次入队失败。出队则是获取head位置，进行cas。

> CAS操作——Compare & Set，或是 Compare & Swap 现在几乎所有的CPU指令都支持CAS的原子操作。
> "原子操作(atomic operation)是不需要synchronized"，这是多线程编程的老生常谈了。 所谓原子操作是指不会被线程调度机制打断的操作；这种操作一旦开始，就一直运行到结束，中间不会有任何context switch （切 换到另一个线程）。

## 10 递归

### 递归需要满足的三个条件
- 一个问题的解可以分解为几个四问题的解
- 这个问题已分解后的子问题，除了数据规模一样，求解思路完全一样
- 存在递归终止条件

### 如何编写递归代码

- **写递归代码的关键就是找到如何将大问题分解为小问题的规律，并基于此写出递推公式，然后找到终止条件，最后将他们翻译成代码**

- **编写递归代码的关键是，只要遇到递归，我们就把它抽象成一盒递推公式，不用想一层层的调用用关系，不要试图用人脑分解递归的每个步骤**

### 需要注意的问题

#### 堆栈溢出

由于函数调用会使用栈来保存临时变量，每调用一个函数都会将临时变量封装为栈帧压入内存栈，等函数执行完成后返回再出栈。这样以来当递归求解数据规模很大，调用层级很深，就会出现堆栈溢出的风险。

#### 警惕重复的计算

求解递归的过程中可能会出现重复计算的问题，这时候可以通过散列表等数据结构来保存计算结果。如果已经求解过了就直接返回，不需要重复计算。

#### 注意空间复杂度

由于递归每调用一次就会在内存堆栈中保存一次现场数据，因此计算递归代码的空间复杂的时候需要考虑着些额外内存开销。

### 递归代码改写成飞递归代码

理论上递归代码都可以改写为非递归代码，但是这样会增加实现的复杂度。

如何做？抽象出递推公式、初始值和边界条件，然后用迭代循环实现。

## 11 排序：冒泡，插入，选择

### 如何分析一个排序算法

#### 执行效率
- 最好情况、最坏情况、平均情况时间复杂度
- 时间复杂度的系数、常数、低阶
- 比较次数和交换、移动次数

#### 内存消耗

原地排序：特指空间复杂度是 O(1)的排序算法。

#### 稳定性

经过排序算法的排序之后，如果原来数据中相同元素的前后顺序没有发生改变，这种算法就称为稳定的排序算法，否则称为不稳定的排序算法。

### 冒泡排序

冒泡排序的原理就是比较相邻元素的大小，符合条件则交换相邻元素的位置，否则继续遍历。

```
void bubbleSort(int *array, int n)
{
    if (n <= 1) return;
    
    for (int i = 0; i < n; ++i) {
        bool change = false;
        for (int j = 0; j < n-i-1; ++j) {
            if (array[j] > array[j+1]) {
                int tmp = array[j+1];
                array[j+1] = array[j];
                array[j] = tmp;
                change = true;
            }
        }
        if (!change) break;
    }
}
```
**编码的过程**

1. 由于每次冒泡只能确定一个最值的位置，因此要想全部排序就需要两个 for 循环
2. 外侧循环表示冒泡的次数，因此循环次数与元素个数 n 相同
3. 内侧循环执行冒泡操作，由于每次冒泡能确定一个元素的最终位置，因此第 i 次冒泡需要操作的元素个数是 n-1-i 个。因此内侧循环从 0 开始 执行 n-i-1 次
4. 如果一次冒泡过程中没有改变元素的位置，只表明元素的顺序已经排好，因此可以通过一个标识标识一次冒泡的过程中是否改变元素位置，如果没有更改就直接退出外侧循环。这样实现了冒泡的优化


#### 冒泡排序是否是原地算法
是，空间复杂度是 O(1)

#### 冒泡排序是否是稳定的排序
是，冒泡的过程中只有当相邻元素的大小不同时才会去改变顺序，相同的元素不会改变顺序。

#### 冒泡排序的时间复杂度

最好情况时间复杂度 O(n)
最坏情况时间复杂度 O(n^2)

平均时间复杂度通过有序度和逆序度分析

- 有序度：数组中具有有序关系的元素对的个数
- 逆序度：数组中具有逆序关系的元素对的个数
- 满有序度：完全有序的数组的有序度

```
逆序度 = 满有序度 - 有序度
```

对于 n 个元素的数组，满有序度为 n*(n-1)/2

最坏情况下有序度为 0，则逆序度为 n*(n-1)/2。这时候要将逆序度减为 0，则需要 n*(n-1)/2 次交换。

因此平均情况下需要 n*(n-1)/4 次交换，因此平均情况复杂度为 O(n^2)

### 插入排序

插入排序的原理就是动态的往有序集合中添加数据。将数组分为已排序区和未排序区，不断将未排序区的元素插入到已排序区中。初始的已排序区只有数组的第一个元素，然后不断添加，直到全部排序完成。

```
void insertionSort(int *array, int n)
{
    if (n <= 1) return;
    
    for (int i = 1; i < n; ++i) {
        int val = array[i];
        int j = i-1;
        for (; j >= 0; --j) {
            if (array[j] > val) {
                array[j+1] = array[j];
            } else {
                break;
            }
        }
        array[j+1] = val;
    }
}
```

**编码的过程**

1. 由于需要把数组的所有元素不断插入已排序区，在插入排序区的过程中需要不断比较插入元素与之前已排序区元素的大小，因此需要两个 for 循环
2. 外侧循环用来遍历所有的元素，因此从 0 开始，循环 n 次
3. 内侧循环用来比较已经选中的元素 array[i] 与已排序区的元素比较，从后往前遍历，因此从 i-1 开始，循环 i-1 次
4. 在内侧循环比较已选中元素 array[i] 与已排序元素时，如果已选中元素满足条件（比如上面的 大于）就替换二者位置，相当于把已选中元素插入到当前已排序元素之前，直到不满足条件说明 选中元素插入到了正确的位置，跳出循环。
5. 内侧循环过一次之后，将 选中的元素插入到内循环结束时的位置，由于内循环体结束时 j 是最终插入的前一个位置，因此需要把选中的元素插入到 j+1 的位置上。

#### 插入排序是否是原地算法
是，空间复杂度是 O(1)

#### 插入排序是否是稳定的排序
是，插入的过程中可以将后面出现的元素插入到前面出现元素的后面，这样以来就不会改变之前相同元素的顺序。

#### 插入排序的时间复杂度

最好是时间复杂度为 O(n)
最坏情况时间复杂度为 O(n^2)

有序数组中插入某个元素的平均时间复杂度是 O(n), 而插入排序需要遍历数组的元素，将每个元素都插入到有序区间中，因此其平均时间复杂度为 O(n^2)

### 选择排序

选择排序通过不断遍历未排序的区域，每次都用一个变量记录最小（大）值的位置，找到位置之后在遍历结束后交换最值与第一个未排序元素的位置

```
void selectionSort(int *array, int n)
{
    if (n <= 1) return;
    
    for (int i = 0; i < n; ++i) {
        int index = i;
        for (int j = i+1; j < n; ++j) {
            if (array[j] < array[index]) {
                index = j;
            }
        }
        if (index != i) {
            int tmp = array[i];
            array[i] = array[index];
            array[index] = tmp;
        }
    }
}
```

**实现步骤**

1. 遍历未排序区查找最值需要一个循环，每次遍历完未排序区之后未排序区就发生了变化，因此需要再次遍历直到未排序区消失，于是要有两个循环嵌套
2. 由于每次内侧循环能确定一个最值的位置，因此外循环从 0 开始 遍历 n 次
3. 内侧循环用来遍历当前未排序区，查找到最值的位置，然后在循环结束之后交换未排序区的第一个元素和最值的位置

#### 选择排序是否是原地算法
是，空间复杂度是 O(1)

#### 选择排序是否是稳定的排序
不是，每次选择最值之后就与未排序区第一个位置交换，这就会打乱之前的相同元素的排序，因此不是稳定的排序。

#### 插入排序的时间复杂度

最好是时间复杂度为 O(n)
最坏情况时间复杂度为 O(n^2)
其平均时间复杂度为 O(n^2)

### 插入与冒泡比较

冒泡排序中每次交换相邻元素位置需要的进行的操作比插入排序的数据移动来说更为耗时，因此插入排序一般优于冒泡排序。

### QA 使用链表实现

>对于老师所提课后题，觉得应该有个前提，是否允许修改链表的节点value值，还是只能改变节点的位置。一般而言，考虑只能改变节点位置，冒泡排序相比于数组实现，比较次数一致，但交换时操作更复杂；插入排序，比较次数一致，不需要再有后移操作，找到位置后可以直接插入，但排序完毕后可能需要倒置链表；选择排序比较次数一致，交换操作同样比较麻烦。综上，时间复杂度和空间复杂度并无明显变化，若追求极致性能，冒泡排序的时间复杂度系数会变大，插入排序系数会减小，选择排序无明显变化。

